{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmhsUefwszwGRtuhxgzcRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatapotturu/CNN-NLP/blob/main/Prompting_for_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpehLXsmgV6V"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "XDveISeFg1R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "genai.configure(api_key=\"_____________________________\")"
      ],
      "metadata": {
        "id": "fAyYFMzXh7TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = '______________________________'\n"
      ],
      "metadata": {
        "id": "CK_AuTXHhBAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"________________________________\"\n"
      ],
      "metadata": {
        "id": "z5_Kk7TihR6n"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "genai.configure(api_key=\"______________________________\")\n"
      ],
      "metadata": {
        "id": "wtYLDks1h0dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "a__9cx1whmTR",
        "outputId": "dd4c7c6f-7273-4c73-90bd-d9c7bd57ef7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a really smart robot friend. This robot can learn things just like you do! It can read books, watch videos, and even play games to learn how things work.\n",
            "\n",
            "This robot is called an AI, which stands for Artificial Intelligence. It's like a super smart computer that can think and solve problems on its own.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "* **Learning:** The AI looks at lots of information and figures out patterns. It's like learning how to recognize different shapes or how to play a game by watching others.\n",
            "* **Thinking:** The AI can use what it's learned to answer questions, make predictions, and even create new things! It's like solving a puzzle or figuring out how to build something cool.\n",
            "* **Helping:** The AI can help us in many ways, like driving our cars, translating languages, or even writing stories!\n",
            "\n",
            "Remember, AI is still learning and growing, just like you are. It's an amazing technology that can make our lives easier and more fun! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "aMPaYkpLiJb_",
        "outputId": "17035d18-2bb7-4334-9d2f-9c121242b859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a really smart robot friend. This robot can learn things just like you do! It can read books, watch videos, and even play games to learn how things work.\n\nThis robot is called an AI, which stands for Artificial Intelligence. It's like a super smart computer that can think and solve problems on its own.\n\nHere's how it works:\n\n* **Learning:** The AI looks at lots of information and figures out patterns. It's like learning how to recognize different shapes or how to play a game by watching others.\n* **Thinking:** The AI can use what it's learned to answer questions, make predictions, and even create new things! It's like solving a puzzle or figuring out how to build something cool.\n* **Helping:** The AI can help us in many ways, like driving our cars, translating languages, or even writing stories!\n\nRemember, AI is still learning and growing, just like you are. It's an amazing technology that can make our lives easier and more fun! \n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = flash.start_chat(history=[])\n",
        "response = chat.send_message('Hello! My name is Zlork.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "U9-qaktoiLYc",
        "outputId": "47744d14-c6c4-4887-e001-3d8c52ff31fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Zlork! It's nice to meet you. What can I do for you today? ðŸ˜Š \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you tell something interesting about dinosaurs?')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "94YYrzjAiQCo",
        "outputId": "f801e13c-8d34-40c1-eccf-2dd95f5175c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bet! Dinosaurs are fascinating creatures. Did you know that some dinosaurs had feathers? \n",
            "\n",
            "**Here's a fun fact:**  The *Microraptor*, a small, feathered dinosaur, was likely able to glide between trees using its feathered wings and legs! \n",
            "\n",
            "What else would you like to know about dinosaurs?  We could talk about their size, their diet, or even the extinction event that wiped them out! ðŸ˜„ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "iOITy6okiYK8",
        "outputId": "22f72b9f-5911-4da3-b7bc-2943521c5fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/aqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if model.name == 'models/gemini-1.5-flash':\n",
        "    print(model)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "32R5XsMoimFS",
        "outputId": "d65425b4-be1b-498c-b37d-0554a6fb35b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
        "\n",
        "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "PiO6KTZ4ipyR",
        "outputId": "3fda3d93-b292-4bd9-cf65-488cbf2f8976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## More Than Just a Snack: The Importance of Olives in Modern Society\n",
            "\n",
            "The olive, a seemingly simple fruit with a long and rich history, holds a surprising significance in modern society. Beyond its delectable taste and culinary versatility, the olive transcends its status as a mere snack, playing vital roles in the economic, environmental, and cultural landscape of our world. This essay will explore the multifaceted importance of olives in modern society, highlighting their contributions to health, nutrition, the economy, the environment, and cultural heritage.\n",
            "\n",
            "**A Culinary Staple and Health Benefactor:**\n",
            "\n",
            "Olives have been a staple food for millennia, and their role in modern cuisine remains strong. From the Mediterranean diet, known for its health benefits, to global culinary creations, olives contribute a unique flavor and texture. Their versatility allows them to be enjoyed as a snack, an ingredient in various dishes, or a key component of salads, pizzas, and tapenade.\n",
            "\n",
            "Beyond taste, olives boast a wealth of health benefits. They\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "iiLwY92OisuC",
        "outputId": "b3e3c022-0e38-48bd-b5c9-3aa282779c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A tiny fruit, a briny bite,\n",
            "A taste of sun, a savory light.\n",
            "From ancient groves to modern plates,\n",
            "The olive's worth, it never fades.\n",
            "\n",
            "A source of oil, a healthy treat,\n",
            "A symbol of peace, a savory feat.\n",
            "From salads fresh to hearty stews,\n",
            "The olive's flavor, always true.\n",
            "\n",
            "On tables grand, in humble homes,\n",
            "Its essence flows, it never roams.\n",
            "A timeless taste, a world's delight,\n",
            "The olive shines, with all its might. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "high_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "\n",
        "for _ in range(3):\n",
        "  response = high_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)\n",
        "\n",
        "  # Slow down a bit so we don't get Resource Exhausted errors.\n",
        "  time.sleep(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "XOA9aXWkiwmf",
        "outputId": "73dc82e9-53c0-49e3-8076-405fe93d1e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Blue \n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
        "\n",
        "for _ in range(4):\n",
        "  response = low_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)\n",
        "\n",
        "  time.sleep(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "ug2UAZFNjRko",
        "outputId": "4ffc5bbe-b09d-4519-d99e-f21d4ac465c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "FefzQqsmkKpv",
        "outputId": "624876db-44a9-4d7f-9fde-9f7e7a43c8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bartholomew, a tabby with eyes like polished amber, was not a cat for routine. He preferred the thrill of the unknown, the rustle of leaves underfoot, the wind whispering secrets in his fur. One sunny afternoon, as Mrs. Higgins, his human, sat engrossed in a novel, Bartholomew saw his chance. He slipped through the cat flap, leaving behind the familiar scent of lavender and old books.\n",
            "\n",
            "The world outside was a symphony of smells - damp earth, blooming honeysuckle, and the pungent musk of a stray dog, whom Bartholomew politely ignored. He padded through the back alleys, his whiskers twitching with anticipation. A rusty fire escape beckoned, and with a graceful leap, he was scaling its heights. From the top, the world spread out before him: rooftops like a terracotta sea, chimneys like miniature towers, and the distant hum of the city, a lullaby to his adventurous soul.\n",
            "\n",
            "A stray feather, carried by the breeze, caught Bartholomew's attention. He followed it, his paws light on the rooftop tiles, until he reached a ramshackle shed, its paint peeling like sunburnt skin. He squeezed through a broken window, the smell of old wood and dust filling his nostrils. Inside, a treasure trove awaited him: a discarded spool of yarn, a chipped teacup filled with rainwater, and a dusty, half-eaten mouse, a gift from a fellow adventurer.\n",
            "\n",
            "Bartholomew, his stomach rumbling with hunger, devoured the mouse, then curled up in the teacup, its cool porcelain a welcome comfort. He dreamt of soaring through the sky, his tail a flag waving in the wind, his amber eyes reflecting the sun's golden rays. But soon, a loud thump and a startled squawk broke his slumber. A young sparrow, its wing trapped in a broken windowpane, was fluttering in distress.\n",
            "\n",
            "Bartholomew, despite his own hunger, felt a surge of empathy. He nudged the bird with his nose, then carefully, with his teeth, gnawed at the broken glass until the sparrow was free. The bird chirped its gratitude, flapping its wings in a flurry of excitement.\n",
            "\n",
            "Bartholomew watched the sparrow take flight, its tiny form a speck against the vastness of the sky. He knew it was time to return. He slipped back out the window, the setting sun painting the sky in hues of orange and purple. He had a story to tell, a tale of adventure, of bravery, and of the joy of helping another in need. He knew, deep in his feline heart, that this was only the beginning of his many, many adventures. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5,\n",
        "    ))\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nVLss6xYkgEW",
        "outputId": "d09f0d60-2421-4fc1-a07d-6b706f645854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: **POSITIVE**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "S0cdDVVNknmV",
        "outputId": "69632784-5166-42d4-9260-4cb701cb0cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ))\n",
        "\n",
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "\n",
        "response = model.generate_content([few_shot_prompt, customer_order])\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "rrKd-Fcak1__",
        "outputId": "39dbde62-eeb1-490a-a819-1b54054a9fa3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "``` \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Yq2RClOQk_1G",
        "outputId": "aa266127-5d29-472e-9250-b3b4debad85c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "zosVgt_xlLEq",
        "outputId": "e947c5bd-bcf2-46ed-d6fe-9633682f3594"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When you were 4, your partner was 3 * 4 = 12 years old.\n",
            "\n",
            "The age difference is 12 - 4 = 8 years.\n",
            "\n",
            "Therefore, your partner is now 20 + 8 = **28 years old**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "MkXDi2-AlMq-",
        "outputId": "ffb5c8a7-0579-434e-b72c-e37b0bcb8e7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how to solve this:\n",
            "\n",
            "* **When you were 4:** Your partner was 3 times your age, which is 4 * 3 = 12 years old.\n",
            "* **Age difference:** This means your partner is 12 - 4 = 8 years older than you.\n",
            "* **Current age:** Since you are now 20, your partner is still 8 years older, making them 20 + 8 = **28 years old**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "X9vq-6NclQgU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "react_chat = model.start_chat()\n",
        "\n",
        "\n",
        "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
        "\n",
        "resp = react_chat.send_message(\n",
        "    [model_instructions, example1, example2, question],\n",
        "    generation_config=config)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "tD_GLpoKlXDk",
        "outputId": "5d652586-47a1-4ac3-c458-51e3358e08f7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to search for the Transformers NLP paper and then look for the author list to find the youngest author.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation, generation_config=config)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "lKnq3bHClkwr",
        "outputId": "a190d21e-c7f3-4cf7-cf0f-17d4037f1c5d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 2\n",
            "I need to find the youngest author from the list of authors in the paper. Unfortunately, the observation only includes names, no ages.  It's not possible to determine the youngest author from just their names.\n",
            "\n",
            "Action 3\n",
            "<finish>I'm sorry, but I can't determine the youngest author based on the information provided. I can only access the information available on Wikipedia.</finish> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ))\n",
        "\n",
        "\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_prompt)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "3iegMFSGlxUk",
        "outputId": "e01fbb08-8257-4041-8df5-f72d4275e2b7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    tools='code_execution')\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_exec_prompt)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "SJvh2OP7l6qT",
        "outputId": "c4ce0547-9f3a-4b0c-986a-8a0a945c6c62"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n``` python\nimport sympy\n\nprimes = sympy.primerange(1, 100)\nodd_primes = [prime for prime in primes if prime % 2 != 0]\nsum_of_primes = sum(odd_primes[:14])\nprint(f'{sum_of_primes=}')\n\n```\n```\nsum_of_primes=326\n\n```\nThe sum of the first 14 odd prime numbers is 326. \n\nHere's how I arrived at this answer:\n\n1. **Identify Prime Numbers:** I used the `sympy.primerange(1, 100)` function to generate a list of prime numbers between 1 and 100.\n2. **Filter for Odd Primes:** I filtered the list of prime numbers to only include odd numbers, which are prime numbers that are not divisible by 2.\n3. **Sum the First 14:** Finally, I summed the first 14 elements of the filtered list, which gives the sum of the first 14 odd prime numbers. \n"
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQJ5ufEcmIdj",
        "outputId": "5da9ab87-20b1-4dda-e8c3-e22946078df9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: \"\"\n",
            "\n",
            "-----\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\nimport sympy\\n\\nprimes = sympy.primerange(1, 100)\\nodd_primes = [prime for prime in primes if prime % 2 != 0]\\nsum_of_primes = sum(odd_primes[:14])\\nprint(f\\'{sum_of_primes=}\\')\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "  output: \"sum_of_primes=326\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "text: \"The sum of the first 14 odd prime numbers is 326. \\n\\nHere\\'s how I arrived at this answer:\\n\\n1. **Identify Prime Numbers:** I used the `sympy.primerange(1, 100)` function to generate a list of prime numbers between 1 and 100.\\n2. **Filter for Odd Primes:** I filtered the list of prime numbers to only include odd numbers, which are prime numbers that are not divisible by 2.\\n3. **Sum the First 14:** Finally, I summed the first 14 elements of the filtered list, which gives the sum of the first 14 odd prime numbers. \\n\"\n",
            "\n",
            "-----\n"
          ]
        }
      ]
    }
  ]
}